{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOIpbrgzSs/9IbpeYx/ItT2",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/andreidemianko/-Nu-shtozh-Sirius-AI/blob/main/Nu_chtozsh.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install python-telegram-bot --upgrade\n",
    "!pip install -q datasets transformers faiss-gpu\n",
    "!pip install nest_asyncio"
   ],
   "metadata": {
    "id": "wh_iHc9_eIme",
    "outputId": "cf805efe-190d-4d5f-d246-9e2399775278",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "metadata": {
      "tags": null
     },
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-telegram-bot\n",
      "  Downloading python_telegram_bot-21.7-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: httpx~=0.27 in /usr/local/lib/python3.10/dist-packages (from python-telegram-bot) (0.27.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx~=0.27->python-telegram-bot) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx~=0.27->python-telegram-bot) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx~=0.27->python-telegram-bot) (1.0.6)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx~=0.27->python-telegram-bot) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx~=0.27->python-telegram-bot) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx~=0.27->python-telegram-bot) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx~=0.27->python-telegram-bot) (1.2.2)\n",
      "Downloading python_telegram_bot-21.7-py3-none-any.whl (654 kB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m654.9/654.9 kB\u001B[0m \u001B[31m11.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hInstalling collected packages: python-telegram-bot\n",
      "Successfully installed python-telegram-bot-21.7\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m480.6/480.6 kB\u001B[0m \u001B[31m5.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.5/85.5 MB\u001B[0m \u001B[31m9.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m116.3/116.3 kB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m179.3/179.3 kB\u001B[0m \u001B[31m10.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m134.8/134.8 kB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m194.1/194.1 kB\u001B[0m \u001B[31m10.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7vbrzpNd7pp"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import nest_asyncio\n",
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes\n",
    "from telegram.ext.filters import PHOTO\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel, PaliGemmaForConditionalGeneration, PaliGemmaProcessor\n",
    "import requests\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n",
    "BOT_TOKEN = \"7335523574:AAFvi2gY4e5ZCRCvQI41AXFSg0DfMQFyvxY\"\n",
    "\n",
    "# Настройка Nest Asyncio для Colab;)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Настройка логирования\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Настройка устройств и моделей\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Настройка модели VLM\n",
    "model_id_vlm = \"google/paligemma-3b-mix-224\"\n",
    "model_vlm = PaliGemmaForConditionalGeneration.from_pretrained(model_id_vlm, torch_dtype=torch.bfloat16)\n",
    "processor_vlm = PaliGemmaProcessor.from_pretrained(model_id_vlm)\n",
    "\n",
    "def describe_picture(input_image):\n",
    "    promt = (\n",
    "        \"Create a detailed description of each item of clothing visible in the image. \"\n",
    "        \"For each item, specify its type, color, pattern (if any), and style, as well as any relevant details like fit or unique features. \"\n",
    "        \"The response should be formatted as a list, with each item description starting with a bullet point (•). \"\n",
    "        \"If there is a person in the photo, do not describe them.\"\n",
    "    )\n",
    "    inputs = processor_vlm(text=promt, images=input_image, padding=\"longest\", do_convert_rgb=True, return_tensors=\"pt\").to(device)\n",
    "    model_vlm.to(device)\n",
    "    inputs = inputs.to(dtype=model_vlm.dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model_vlm.generate(**inputs, max_length=496)\n",
    "\n",
    "    decoded_output = processor_vlm.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    if decoded_output.startswith(promt):\n",
    "        decoded_output = decoded_output[len(promt):].strip()\n",
    "\n",
    "    return decoded_output\n",
    "\n",
    "\n",
    "# Загрузка и обработка датасета\n",
    "dataset = load_dataset(\"ceyda/fashion-products-small\", split=\"train\")\n",
    "\n",
    "# Функция для создания эмбеддингов изображений\n",
    "def create_image_embeddings(dataset):\n",
    "    image_embeddings = []\n",
    "    image_paths = []\n",
    "\n",
    "    for item in dataset:\n",
    "        try:\n",
    "            image_url = item[\"link\"]\n",
    "            image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n",
    "            inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.get_image_features(**inputs)\n",
    "                embedding /= embedding.norm(p=2, dim=-1, keepdim=True)\n",
    "            image_embeddings.append(embedding.cpu().numpy())\n",
    "            image_paths.append(image_url)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка при обработке изображения {image_url}: {e}\")\n",
    "\n",
    "    return np.vstack(image_embeddings), image_paths\n",
    "\n",
    "# Создание или загрузка эмбеддингов для датасета\n",
    "if os.path.exists(\"image_embeddings.npy\") and os.path.exists(\"image_paths.npy\"):\n",
    "    image_embeddings = np.load(\"image_embeddings.npy\")\n",
    "    image_paths = np.load(\"image_paths.npy\", allow_pickle=True)\n",
    "else:\n",
    "    subset = dataset.select(range(500))\n",
    "    image_embeddings, image_paths = create_image_embeddings(subset)\n",
    "    np.save(\"image_embeddings.npy\", image_embeddings)\n",
    "    np.save(\"image_paths.npy\", image_paths)\n",
    "\n",
    "# Создание индекса для быстрого поиска\n",
    "dimension = image_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(image_embeddings)\n",
    "\n",
    "# Функция для поиска наиболее подходящего изображения для каждого описания\n",
    "def search_similar_images_by_description(descriptions, top_k=1):\n",
    "    results = []\n",
    "\n",
    "    for description in descriptions:\n",
    "        inputs = clip_processor(text=[description], return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_embedding = clip_model.get_text_features(**inputs)\n",
    "            text_embedding /= text_embedding.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "        D, I = index.search(text_embedding.cpu().numpy(), top_k)\n",
    "        similar_image = image_paths[I[0][0]]  # Получаем URL изображения с наивысшим баллом\n",
    "        results.append((description, similar_image))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Обработчик изображений\n",
    "async def handle_image(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    if update.message.photo:\n",
    "        try:\n",
    "            photo_file = await update.message.photo[-1].get_file()\n",
    "            photo_bytes = await photo_file.download_as_bytearray()\n",
    "            input_image = Image.open(BytesIO(photo_bytes)).convert('RGB')\n",
    "\n",
    "            # Генерация описания с помощью VLM модели\n",
    "            description = describe_picture(input_image)\n",
    "            await update.message.reply_text(f\"Описание изображения: {description}\")\n",
    "\n",
    "            # Разделение описания на отдельные элементы, если это список строк\n",
    "            descriptions_list = description.split(';')  # Измените разделитель в зависимости от формата выхода VLM\n",
    "\n",
    "            # Поиск изображений, соответствующих каждому описанию\n",
    "            search_results = search_similar_images_by_description(descriptions_list)\n",
    "\n",
    "            for desc, img_url in search_results:\n",
    "                await update.message.reply_text(f\"Для описания \\\"{desc}\\\", найдено изображение:\")\n",
    "                await update.message.reply_photo(img_url)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ошибка при обработке изображения: {e}\")\n",
    "            await update.message.reply_text(\"Произошла ошибка при обработке изображения. Пожалуйста, попробуйте снова.\")\n",
    "\n",
    "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    await update.message.reply_text(\n",
    "        'Привет! Отправь мне изображение одежды, и я найду похожие товары из ceyda/fashion-products-small.'\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    application = ApplicationBuilder().token(BOT_TOKEN).build()\n",
    "\n",
    "    application.add_handler(CommandHandler(\"start\", start))\n",
    "    application.add_handler(MessageHandler(PHOTO, handle_image))\n",
    "\n",
    "    logger.info(\"Бот запущен и готов к работе...\")\n",
    "    application.run_polling()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ]
}
